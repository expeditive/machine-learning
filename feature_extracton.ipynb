{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKnyL9OMaT7C7KgBiArYEP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/expeditive/machine-learning/blob/main/feature_extracton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature extraction** in Machine Learning (ML) is the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set. It‚Äôs a key part of **feature engineering** and often determines the success of your model.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What is Feature Extraction?\n",
        "Feature extraction involves:\n",
        "- Selecting important aspects of data.\n",
        "- Converting data into a format that is suitable for ML models.\n",
        "- Reducing data dimensionality (optional but common).\n",
        "- Improving model accuracy and training time.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why is Feature Extraction Important?\n",
        "- Helps models learn better from data.\n",
        "- Reduces noise and irrelevant data.\n",
        "- Speeds up training a\n",
        "nd improves accuracy.\n",
        "- Makes it possible to use complex data types (like images, text, audio).\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Common Feature Extraction Techniques\n",
        "\n",
        "#### 1. **For Numerical Data**\n",
        "- **Statistical features**: mean, standard deviation, min, max, etc.\n",
        "- **Polynomial features**: adding interactions between variables.\n",
        "- **Normalization/Standardization**: scaling data to a standard range.\n",
        "\n",
        "#### 2. **For Text Data**\n",
        "- **Bag of Words (BoW)**\n",
        "- **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "- **Word Embeddings**: Word2Vec, GloVe, BERT\n",
        "\n",
        "#### 3. **For Image Data**\n",
        "- **Pixel intensity values**\n",
        "- **Edge detection**: using filters like Sobel, Canny\n",
        "- **CNN features**: using layers of a Convolutional Neural Network (deep learning)\n",
        "\n",
        "#### 4. **For Audio Data**\n",
        "- **MFCC (Mel Frequency Cepstral Coefficients)**\n",
        "- **Chroma features**\n",
        "- **Spectrograms**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Feature Extraction vs Feature Selection\n",
        "- **Feature Extraction** = creating new features from raw data (e.g., PCA, TF-IDF)\n",
        "- **Feature Selection** = choosing a subset of existing features that are most relevant (e.g., using correlation, mutual information)\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Tools/Libraries Used\n",
        "- **Python**: `scikit-learn`, `pandas`, `NumPy`\n",
        "- **For NLP**: `NLTK`, `spaCy`, `transformers`\n",
        "- **For images**: `OpenCV`, `TensorFlow`, `PyTorch`\n",
        "\n",
        "--"
      ],
      "metadata": {
        "id": "eoYTLr-Uk4wj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† TF-IDF Vectorizer in Machine Learning\n",
        "\n",
        "**TF-IDF (Term Frequency - Inverse Document Frequency)** is a popular technique for **feature extraction from text data**. It transforms text into numerical vectors that reflect how important a word is to a document **relative to a collection (corpus)**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìö Breakdown of TF-IDF\n",
        "\n",
        "#### ‚úÖ 1. **Term Frequency (TF)**  \n",
        "Measures how frequently a term appears in a document.\n",
        "\n",
        "\\[\n",
        "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total terms in document } d}\n",
        "\\]\n",
        "\n",
        "#### ‚úÖ 2. **Inverse Document Frequency (IDF)**  \n",
        "Measures how important a term is in the **entire corpus**. Rare terms across all documents get higher scores.\n",
        "\n",
        "\\[\n",
        "\\text{IDF}(t) = \\log\\left(\\frac{\\text{Total number of documents}}{\\text{Number of documents with term } t}\\right)\n",
        "\\]\n",
        "\n",
        "#### ‚úÖ 3. **TF-IDF Score**  \n",
        "\\[\n",
        "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Using `TfidfVectorizer` in Python (from `sklearn`)\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "corpus = [\n",
        "    \"Machine learning is fascinating\",\n",
        "    \"Learning algorithms are essential for machine learning\",\n",
        "    \"Deep learning is a subset of machine learning\"\n",
        "]\n",
        "\n",
        "# Initialize vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Show feature names (vocabulary)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Convert to dense array and view TF-IDF scores\n",
        "print(X.toarray())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ When to Use TF-IDF:\n",
        "- For **text classification** (e.g., spam detection, sentiment analysis)\n",
        "- To reduce noise from **common words** (e.g., \"the\", \"and\", etc.)\n",
        "- For **search engines** and **document similarity** tasks\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Tips:\n",
        "- Combine with dimensionality reduction (e.g., TruncatedSVD for LSA)\n",
        "- Use **stop words** removal to ignore common words\n",
        "- Adjust parameters like `max_df`, `min_df`, `ngram_range` for better performance\n",
        "\n",
        "---\n",
        "\n",
        "Want to see a real-world example like spam classification using TF-IDF + ML model?"
      ],
      "metadata": {
        "id": "w2Vy3yWOmS-F"
      }
    }
  ]
}