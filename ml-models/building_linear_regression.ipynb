{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGYMZOAs9fSgsyBZksY0ml",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/expeditive/machine-learning/blob/main/ml-models/building_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear reagression**\n",
        "\n",
        "*Y = wX + b*\n",
        "\n",
        "Y --> dependent variable\n",
        "\n",
        "X --> independent variable\n",
        "\n",
        "w --> weight\n",
        "\n",
        "b --> bias\n",
        "\n",
        "**gradient descent**\n",
        "\n",
        "gradient descent is an optimization algorithm used for minimizing the loss function various machine learning algorithms. it is used for updating the parameters of the learning model.\n",
        "\n",
        "\n",
        "\n",
        "w = w - @*dw\n",
        "\n",
        "b = b - @*db\n",
        "\n",
        "**Learning Rate**\n",
        "\n",
        "learning rate is tuning parametre in an optimization algorithm that determines the step size at each function while moving toward a minimum of a loss function"
      ],
      "metadata": {
        "id": "6BP0jfm-8K4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing numpy lib\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "epr99UgI961u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**linear regression**"
      ],
      "metadata": {
        "id": "ykVBLzWgAgPY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Regression():\n",
        "\n",
        "  #initiating the  hyperparameters\n",
        "  def __init__(self,learning_rate, no_of_iterations):\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.no_of_iterations = no_of_iterations\n",
        "\n",
        "  def fit(self,X ,Y ):\n",
        "\n",
        "    #no of training examples and number of features\n",
        "\n",
        "    self.m , self.n = X.shape() #number of rows and columns\n",
        "\n",
        "    #initiating the weight and bias\n",
        "\n",
        "    self.w = np.zeros(self.n)\n",
        "    self.b = 0\n",
        "    self.X = X\n",
        "    self.Y = Y\n",
        "\n",
        "    #implementing gradient descent\n",
        "\n",
        "    for i in range(self.no_of_iterations):\n",
        "      self.update_wieghts()\n",
        "\n",
        "\n",
        "  def update_wieghts(self, ):\n",
        "\n",
        "    Y_prediction = self.predict(self.X)\n",
        "\n",
        "    #calculate gradients\n",
        "\n",
        "    dw = - (2 * (self.X.T).dot(self.Y - Y_prediction)) / self.m\n",
        "\n",
        "    db = - 2 * np.sum(self.Y - Y_prediction) / self.m\n",
        "\n",
        "    #updating the weights\n",
        "\n",
        "    self.w = self.w - self.learning_rate*dw\n",
        "    self.b = self.b - self.learning_rate*db\n",
        "\n",
        "\n",
        "\n",
        "  def predict(self, X ):\n",
        "\n",
        "    return X.dot(self.w) + self.b\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D5lQQAfeAfi0"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}